## GPUs from Nvidia
- [Nvidia's latest GPU](https://www.nvidia.com/en-us/data-center/tesla-v100/) can do 15 TFlops of SP or 120 TFlops with its new Tensor core architecture which is a FP16 multiply and FP32 accumulate or add to suit ML.
- Nvidia is packing up 8 boards into their [DGX-1](https://www.nvidia.com/en-us/data-center/dgx-server/)for 960 Tensor TFlops.
- [Nvidia Volta - 架构看点](https://mp.weixin.qq.com/s/tEX4H7OEbZF4dKMI0ZOPmw) gives some insights of Volta architecture.

## SoC from Nvidia

## Open Source DLA from Nvidia

## GPUs from AMD
- The soon to be released [AMD Radeon Instinct MI25](https://instinct.radeon.com/en-us/product/mi/radeon-instinct-mi25/) is promising 12.3 TFlops of SP or 24.6 TFlops of FP16. If your calculations are amenable to Nvidia's Tensors, then AMD can't compete. Nvidia also does twice the bandwidth with 900GB/s versus AMD's 484 GB/s.

## Intel

## Xilinx FPGA

## Google' TPU

## Qualcomm

## Apple

## IBM TrueNorth

## Wave Computing

## Graphcore

## Cambricon

## Deephi

## Bitmain

## Horizon Robotics

## KnuEdge's KnuPath

## PEZY Computing K.K.

## KnuEdge's KnuPath

## Tenstorrent

## Cerebras

## Thinci

## Koniku

## Adapteva

## Knowm

## Mythic

## Kalray

## Groq

## Aimotive

## Deep Vision

## Deep Scale

## REM

## Leepmind

## KAIST DNPU

## MIT A Scalable Speech Recognizer

## Synopsys Embedded Vision

## CEVA XM6

## VeriSilicon VIP8000

## Cadence P5/P6/C5

### Reference
1. [FPGAs and AI processors: DNN and CNN for all](https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html)

